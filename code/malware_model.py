
# coding: utf-8

# In[1]:


try:
    import cudf as gd
    import nvstrings
    from librmm_cffi import librmm
    from nvstring_workaround import get_unique_tokens,on_gpu,get_token_counts,is_in
    from cudf_workaround import unique,rename_col
except:
    print('cudf not imported')


# In[2]:


import time
from ml_robot import timer,bag_xgb_model,xgb_model,lgb_model,bag_lgb_model,write_log
from ml_robot.metrics import get_score
import os
import pandas as pd
import numpy as np
from collections import OrderedDict,Counter
import sys
from ffm_model import FFM

# In[3]:


WORK = '/raid/data/ml/malware'
CACHE = '%s/code2/cache'%WORK
PATH = '%s/input'%WORK

MODE = None
METRIC = 'auc'
GPU = 0
if len(sys.argv)==2 and sys.argv[0].endswith('.py'):
    GPU = int(sys.argv[1])
    print('Reset GPU',GPU)
os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU)
print("GPU:",GPU)


# In[4]:


IDCOL = 'MachineIdentifier'
YCOL = 'HasDetections'


# # Functions

# In[5]:


def mkdir(path):
    if os.path.exists(path)==0:
        os.mkdir(path)

@timer
def get_dtype(path,nrows=10):
    """get data type for cudf.read_csv
    by using pandas's read_csv with a small number of rows.

    Parameters
    ----------
    path : str, path of the csv file
    nrows : int, default: 10
        number of rows to read for pd.read_csv

    Returns
    ----------
    col2dtype : dictionary, column name => data type
    """
    if nrows is not None:
        train = pd.read_csv(path,nrows=nrows)
    else:
        train = pd.read_pickle(path.replace('.csv','.pkl'))
    col2dtype = OrderedDict()
    for col in train.columns:
        if train[col].dtype=='O':
            col2dtype[col] = 'str'
        elif train[col].dtype==np.int64:
            col2dtype[col] = 'int32'
        else:
            col2dtype[col] = 'float32'
    return col2dtype

@timer
def read_csv_hash_nvstring(path,cols=None):
    """read a csv file to a dictionary of nvstring objects
    Parameters
    ----------
    path : str, path of the csv file
    cols : list, a list of column names to be read

    Returns
    ----------
    str_cols : list of string column name 
    df: gd.dataframe
    """
    dic = get_dtype(path,1000)
    col_names = [i for i in dic]
    dtypes = [dic[i] for i in col_names]

    gd_cols = gd.io.csv.read_csv_strings(path,
                        names=col_names, dtype=dtypes,
                        skiprows=1)
    str_cols = [] 
    df = gd.DataFrame()
    for name,dtype,ds in zip(col_names,dtypes,gd_cols):
        if cols is not None and name not in cols:
            continue
        if dtype =='str':
            df[name] = on_gpu(ds,'hash')
            str_cols.append(name)
        else:
            df[name] = ds
            
    del gd_cols
    return df,str_cols

@timer
def read_csv_with_nvstring(path,cols=None):
    """read a csv file to a dictionary of nvstring objects
    Parameters
    ----------
    path : str, path of the csv file
    cols : list, a list of column names to be read

    Returns
    ----------
    str_cols : dictionary, column name => nvstring object
    df: gd.dataframe
    """
    dic = get_dtype(path,1000)
    col_names = [i for i in dic]
    dtypes = [dic[i] for i in col_names]

    gd_cols = gd.io.csv.read_csv_strings(path,
                        names=col_names, dtype=dtypes,
                        skiprows=1)
    str_cols = {} # column name => nvstring object
    df = gd.DataFrame()
    for name,dtype,ds in zip(col_names,dtypes,gd_cols):
        if cols is not None and name not in cols:
            continue
        if dtype =='str':
            str_cols[name] = ds
        else:
            df[name] = ds
    del gd_cols
    return df,str_cols

@timer
def read_csv_hash(path):
    """read a csv file by hashing the str cols
    Parameters
    ----------
    path : str, path of the csv file

    Returns
    ----------
    df: gd.dataframe
    """
    dic = get_dtype(path,1000)
    col_names = [i for i in dic]
    dtypes = [dic[i] for i in col_names]
    str_cols = [i for i in col_names if dic[i]=='str'] 
    dtypes = ['int32' if i=='str' else i for i in dtypes]

    gdf = gd.read_csv(path,names=col_names,dtype=dtypes,skiprows=1)
    return gdf,str_cols

@timer
def split():
    mkdir(CACHE)
    out = '%s/tr.csv'%CACHE
    if os.path.exists(out):
        return
    gdf,ds = read_csv_with_nvstring('%s/train.csv'%PATH)
    col = 'AvSigVersion'
    nv = ds[col]
    nvs = nv.split_column('.')
    c = 1 # use the 2nd digit (0 based) to split train data.
    xcol = '%s_%d'%(col,c)    
    gdf[xcol] = on_gpu(nvs[c],'stoi')
    df = gdf.to_pandas()
    tmp = pd.read_csv('%s/train.csv'%PATH,nrows=10)
    cols = [i for i in tmp.columns]
    for col in ds:
        df[col] = ds[col].to_host()
    df = df[cols+[xcol]] # reorder the columns
    mask = df[xcol]>=275
    df['random'] = np.random.random(df.shape[0])
    random_mask = df.random<0.13
    print(df[mask].shape,df[~mask].shape)
    mask = mask | random_mask
    print(df[mask].shape,df[~mask].shape)
    del ds
    del gdf
    del nv,nvs
    df[~mask].drop('random',axis=1).to_csv(out,index=False)
    df[mask].drop('random',axis=1).to_csv(out.replace('tr','va'),index=False)


# In[6]:


@timer
def get_id_y(mode):
    if mode == 'cv':
        tr,te = 'tr','va'
    else:
        tr,te = 'train','test'
    out = '%s/%s_y.pkl'%(CACHE,tr)
    if os.path.exists(out):
        y = pd.read_pickle(out)[YCOL].values
        if mode == 'cv':
            yt = pd.read_pickle(out.replace(tr,te))[YCOL].values
        else:
            yt = None
        out = out.replace('_y','_id')
        tr_id = pd.read_pickle(out)[IDCOL].values
        te_id = pd.read_pickle(out.replace(tr,te))[IDCOL].values
        return y,yt,tr_id,te_id

    tr_path,te_path = get_tr_te_paths(mode)
    gtr_y,gtr_id = read_csv_with_nvstring(tr_path,[IDCOL,YCOL])
    gte_y,gte_id = read_csv_with_nvstring(te_path,[IDCOL,YCOL])
    
    y = gtr_y[YCOL].to_array()
    pd.DataFrame({YCOL:y}).to_pickle(out)
    #pd.DataFrame({YCOL:y}).to_csv(out,index=False,compression='gzip')
    del gtr_y
    
    if mode == 'cv':
        yt = gte_y[YCOL].to_array()
        pd.DataFrame({YCOL:yt}).to_pickle(out.replace(tr,te))
        #pd.DataFrame({YCOL:yt}).to_csv(out.replace(tr,te),index=False,compression='gzip')
    else:
        yt = None
    del gte_y
    
    out = out.replace('_y','_id')
    
    tr_id = np.array(gtr_id[IDCOL].to_host())
    pd.DataFrame({IDCOL:tr_id}).to_pickle(out)
    #pd.DataFrame({IDCOL:tr_id}).to_csv(out,index=False,compression='gzip')
    del gtr_id
    
    te_id = np.array(gte_id[IDCOL].to_host())
    pd.DataFrame({IDCOL:te_id}).to_pickle(out.replace(tr,te))
    #pd.DataFrame({IDCOL:te_id}).to_csv(out.replace(tr,te),index=False,compression='gzip')
    del gte_id
    
    return y,yt,tr_id,te_id


# In[7]:


def get_tr_te_paths(mode):
    if mode == 'cv':
        tr_path = '%s/tr.csv'%CACHE
        te_path = '%s/va.csv'%CACHE
    else:
        tr_path = '%s/train.csv'%PATH
        te_path = '%s/test.csv'%PATH
    return tr_path,te_path

def rm_cols(gdf,cols):
    gcols = [i for i in gdf.columns]
    for col in cols:
        if col in gcols:
            del gdf[col]
    return gdf

def exist(i,mode):
    out = '%s/fea%s_%d_tr.pkl'%(CACHE,mode,i)
    return os.path.exists(out)

def load(i,mode):
    out = '%s/fea%s_%d_tr.pkl'%(CACHE,mode,i)
    x = pd.read_pickle(out)
    xt = pd.read_pickle(out.replace('tr','te'))
    cols = [i for i in x.columns]
    return x.values,xt.values,cols

def rm(i,mode):
    out = '%s/fea%s_%d_tr.pkl'%(CACHE,mode,i)
    if os.path.exists(out):
        os.remove(out)
    out = out.replace('tr','te')
    if os.path.exists(out):
        os.remove(out)

def dump(i,mode,x,xt,cols):
    out = '%s/fea%s_%d_tr.pkl'%(CACHE,mode,i)
    pd.DataFrame(x,columns=cols).to_pickle(out)
    pd.DataFrame(xt,columns=cols).to_pickle(out.replace('tr','te'))

    
    
@timer
def build(mode,build_list=[1],cache=[0]):
    assert len(build_list) == len(cache)
    y,yt,tr_id,te_id = get_id_y(mode)
    if len(build_list)==0:
        return None,None,y,yt,None,tr_id,te_id
    X,Xt,names = [],[],[]
    for i,s in zip(build_list,cache):
        if s and exist(i,mode):
            x,xt,cols = load(i,mode)
        else:
            x,xt,cols = eval('build%d(mode)'%i)
            if s:
                dump(i,mode,x,xt,cols)
            else:
                rm(i,mode)
        X.append(x)
        Xt.append(xt)
        names.append(cols)
    X = np.hstack(X)
    Xt = np.hstack(Xt)
    names = [i for cols in names for i in cols]
    return X,Xt,y,yt,names,tr_id,te_id   
      
@timer 
def gdf_factorize(gtr,strcols):
    for col in strcols:
        if col in gtr.columns:
            gtr[col],_ = gtr[col].fillna(0).factorize()
    return gtr

@timer
def build1(mode):
    tr_path,te_path = get_tr_te_paths(mode)
    gtr,strcols = read_csv_hash_nvstring(tr_path)
    gte,_ = read_csv_hash_nvstring(te_path)
    badcols = ['AvSigVersion','PuaMode']
    gtr = rm_cols(gtr,[IDCOL,YCOL]+badcols)
    gte = rm_cols(gte,[IDCOL,YCOL]+badcols)
    return post_gdf(gtr,gte)   



def mtr(tr,col,te):
    #print('0')
    tr[col] = tr[col].fillna(-999)
    #print('1')
    tr[YCOL] = tr[YCOL].astype('float32')
    tr[col] = tr[col].astype('int32')
    te[col] = te[col].astype('int32')
    #print('2')
    dg = tr.groupby(col).agg({YCOL:'mean'})
    #print('3')
    dg = rename_col(dg,'mean_%s'%YCOL,'mtr_%s'%col)
    #print('4')
    if len(dg)>2:
        #tr = tr.merge(dg,on=[col],how='left')
        te = te.merge(dg,on=[col],how='left')
        #ptr = dg.to_pandas()
        #pte = te.to_pandas()
        #del dg,te
        #dg = pd.DataFrame.from_pandas(ptr)
        #te = pd.DataFrame.from_pandas(pte)
        
    del dg
    return te

@timer
def mtr_encode(tr,te,cols):
    tr = rm_cols(tr,[i for i in tr.columns if i not in cols+[YCOL]])
    te = rm_cols(te,[i for i in te.columns if i not in cols+[YCOL]])
    tr['idx'] = np.arange(len(tr))
    tr['random'] = np.random.random(len(tr))
    tr = tr.sort_values(by='random')
    tr.drop_column('random') 
    N = len(tr)//2
    tr1 = tr[:N]
    tr2 = tr[N:]
    te['idx'] = np.arange(len(te))
    for col in cols:
        if col not in tr.columns:
            continue
        #print('mtr tr1')
        tr2 = mtr(tr1,col,tr2)
        #tr2.to_pandas().to_pickle('tr2.pkl')
        #tr1.to_pandas().to_pickle('tr1.pkl')
        #print('mtr tr2')
        tr1 = mtr(tr2,col,tr1)
        #print('mtr te')
        te = mtr(tr,col,te)
    if len(tr1.columns)!=len(tr2.columns):
        del tr1,tr2
        return rm_cols(tr,[i for i in tr.columns]),rm_cols(te,[i for i in te.columns])
    del tr
    tr = gd.concat([tr1,tr2])
    tr = tr.sort_values(by='idx')
    tr = rm_cols(tr,[i for i in tr.columns if i.startswith('mtr_')==0])
    del tr1,tr2
    te = te.sort_values(by='idx')
    te = rm_cols(te,[i for i in te.columns if i.startswith('mtr_')==0])
    return tr,te

@timer
def count_encode(df,cols):
    df['idx'] = np.arange(len(df))
    for col in cols:
        if col not in df.columns:
            continue
        df[col] = df[col].fillna(0)
        dg = df.groupby(col).agg({col:'count'})
        if len(dg)>2:
            df = df.merge(dg,on=[col],how='left')
            df.drop_column(col)
        del dg
    df = df.sort_values(by='idx')
    df.drop_column('idx')
    df = rm_cols(df,[i for i in df.columns if i.startswith('count_')==0])
    return df

def reset_col_dtype(gtr,gte):
    for col in gtr.columns:
        a,b = str(gtr[col].dtype),str(gte[col].dtype)
        if a!=b:
            gtr[col] = gtr[col].astype('float32')
            gte[col] = gte[col].astype('float32')
    return gtr,gte

@timer
def build0(mode):
    tr_path,te_path = get_tr_te_paths(mode)
    gtr,_ = read_csv_hash(tr_path)
    gte,_ = read_csv_hash(te_path)
    gtr = rm_cols(gtr,[IDCOL,YCOL])
    gte = rm_cols(gte,[IDCOL,YCOL])
    N = len(gtr)
    gtr,gte = reset_col_dtype(gtr,gte)
    df = gd.concat([gtr,gte])
    del gtr,gte
    num_cols = get_true_numerical_columns()    
    str_cols = [i for i in df.columns if i not in num_cols]
    df = df[str_cols+num_cols]
    df = gdf_factorize(df,str_cols)     
    data = df.to_pandas()
    del df
    cols = [i for i in data.columns]
    data = data.values
    print("# of ",data.shape)
    return data[:N],data[N:],cols

@timer
def build2(mode):
    tr_path,te_path = get_tr_te_paths(mode)
    gtr,_ = read_csv_hash(tr_path)
    gte,_ = read_csv_hash(te_path)
    gtr = rm_cols(gtr,[IDCOL,YCOL])
    gte = rm_cols(gte,[IDCOL,YCOL])
    N = len(gtr)
    gtr,gte = reset_col_dtype(gtr,gte)
    df = gd.concat([gtr,gte])
    del gtr,gte
    #badcols = [i for i in gtr.columns]
    count_cols = ['Census_ProcessorManufacturerIdentifier'] 
    print("# of count cols",len(count_cols))
    df = count_encode(df,count_cols)
    data = df.to_pandas()
    del df
    cols = [i for i in data.columns]
    data = data.values
    print("count shape",data.shape)
    return data[:N],data[N:],cols

@timer
def build3(mode):
    tr_path,te_path = get_tr_te_paths(mode)
    gtr,_ = read_csv_hash(tr_path)
    gte,_ = read_csv_hash(te_path)
    gtr = rm_cols(gtr,[IDCOL])
    gte = rm_cols(gte,[IDCOL,YCOL])
    mtr_cols = ['AVProductStatesIdentifier','CountryIdentifier','CityIdentifier'] 
    print("# of mtr cols",len(mtr_cols))
    gtr,gte = mtr_encode(gtr,gte,mtr_cols)
    ptr = gtr.to_pandas()
    pte = gte.to_pandas()
    cols = [i for i in ptr.columns]
    del gtr,gte
    return ptr.values,pte.values,cols

def post_gdf(gtr,gte):
    tr = gtr.to_pandas().values
    te = gte.to_pandas().values
    cols = [i for i in gtr.columns]
    del gtr,gte
    return tr,te,cols

    """
    for col in gtr.columns:
        a,b = str(gtr[col].dtype),str(gte[col].dtype)
        if a!=b:
            gtr[col] = gtr[col].astype('float32')
            gte[col] = gte[col].astype('float32')
    """


# In[8]:


def get_model(names,num_class,model='nn'):
    model = eval('get_%s_model(names,num_class)'%model)
    return model

def get_categorical_columns(names):
    true_numerical_columns = get_true_numerical_columns()
    true_numerical_columns += get_new_numerical_columns()
    #true_numerical_columns += get_binary_columns()
    return [i for i in names if i not in true_numerical_columns and i.startswith('mtr_')==False and i.startswith('count_')==False]

def get_true_numerical_columns():
    true_numerical_columns = [
        'Census_ProcessorCoreCount',
        'Census_PrimaryDiskTotalCapacity',
        'Census_SystemVolumeTotalCapacity',
        'Census_TotalPhysicalRAM',
        'Census_InternalPrimaryDiagonalDisplaySizeInInches',
        'Census_InternalPrimaryDisplayResolutionHorizontal',
        'Census_InternalPrimaryDisplayResolutionVertical',
        'Census_InternalBatteryNumberOfCharges'
    ]
    return true_numerical_columns

def get_new_numerical_columns():
    cols = 'AvSigVersion_1,'.split(',')
    return cols

def get_xgb_cpu_params(names,num_class):
    print("# clases",num_class)
    #categorical = get_categorical_columns(names)
    params =  {
        'objective': 'binary:logistic', 
        'tree_method': 'hist', 
        'early_stopping_rounds':None,
        'eta':0.1,
        'nthread': 16, 
        'folds':100,
        'watch':False,
        'num_class':num_class,
        'num_round':200,
        'max_depth': 10, 
        'silent':1,
        'subsample':0.8,
        'colsample_bytree': 0.7,
        'min_child_weight':10,
        'feature_names':names,
        'maximize':True,
        'eval_metric':METRIC,
        'verbose_eval':100,
    }
    return params

def get_xgb_gpu_params(names,num_class):
    params = get_xgb_cpu_params(names,num_class)
    params.update({'tree_method': 'gpu_hist',
        #'objective': 'reg:linear',
    })
    return params

def get_xgb_params(names,num_class):
    #params = get_xgb_cpu_params(names,num_class)
    params = get_xgb_gpu_params(names,num_class)
    return params

def get_xgb_model(names,num_class):
    xgb_params = get_xgb_params(names,num_class)
    model = xgb_model(**xgb_params)
    return model

def get_bag_xgb_model(names,num_class):
    xgb_params = get_xgb_params(names,num_class)
    xgb_params['folds'] = 4
    xgb_params['num_round'] = 500
    #xgb_params['ranknorm'] = True
    model = bag_xgb_model(**xgb_params)
    return model

def get_lgb_model(names,num_class):
    lgb_params = get_lgb_params(names,num_class)
    model = lgb_model(**lgb_params)
    return model

def get_bag_lgb_model(names,num_class):
    lgb_params = get_lgb_params(names,num_class)
    lgb_params['folds'] = 4
    lgb_params['num_round'] = 1000
    model = bag_lgb_model(**lgb_params)
    return model

def get_lgb_params(names,num_class):
    print("# clases",num_class)
    categorical = get_categorical_columns(names)
    print("categorical",categorical)
    lgbm_params =  {
        'num_threads':16,
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': METRIC,#None,#'multi_logloss',
        'num_round': 10000,
        'folds':1000,
        'num_class':num_class,
        "early_stopping_rounds":100,
        "feature_name": names,
        #'num_leaves': 16,
        'seed': 3,
        #'load_model':'backup/2018-05-29-19-56-01_cv_0.2215/lgb_sub.model',
    
        'learning_rate': 0.1,
        #'max_bin':32,
        'subsample': .9,
        'colsample_bytree': 0.7,
        'reg_alpha': .01,
        'reg_lambda': .01,
        'min_split_gain': 0.01,
        'min_child_weight': 10,
        #'silent': -1,
        'max_depth': None,    

        'verbosity':-1,
        'importance_type': 'gain',
        'stratified':False,
        'verbose': -1,
        'verbose_eval':100,
        'categorical': [],#categorical
    }
    return lgbm_params


# In[9]:
def get_ffm_model(names=None,num_class=None):
    params = {
        'mode': MODE,
        'cache':'%s/ffm1'%CACHE,
        'path': CACHE if MODE=='cv' else PATH,
        'fmap':'fmap.txt',
        'autostop':3,
        'metric':'auc',
        'threads':64,
        'embedding_size':4,
        'lr':0.1,
        'epochs':100,
        'lambda':0.0001,
    }
    if MODE == 'sub':
        params['autostop'] = params['epochs']
    model = FFM(**params)
    return model

@timer
def fit_predict(names,X,y,Xt,yt=None,model_name='lgb'):
    num_class = 1
    model = get_model(names,num_class,model_name)
    if model_name == 'ffm':
        yp = model.fit_predict()
        return yp,model 
    if 'xgb' not in model_name and yt is not None:
         model.fit(X,y,va=[Xt,yt])
    else:
         model.fit(X,y)#,va=None)
    yp = model.predict(Xt)
    return yp,model

def main(mode,model='lgb'):
    global MODE
    MODE = mode 
    start = time.time()
    tag = '%s_%s_%d'%(mode,model,GPU)
    out = '%s.csv.gz'%tag
    bl = [1]
    cl = [0]*len(bl)
    if model == 'ffm':
        bl,cl = [],[]
    X,Xt,y,yt,names,tr_id,te_id = build(mode,build_list=bl,cache=cl)
    yp,model = fit_predict(names,X,y,Xt,yt,model_name=model)
    try:
        model.get_importance("%s_importance.csv"%tag)
    except:
        pass
    if mode == 'cv':
        score = get_score(yt,yp,METRIC)
    else:
        score = -1 #None
    print(METRIC,score)
    if mode == 'sub':
        s = pd.DataFrame({IDCOL:te_id,YCOL:yp})
    else:
        s = pd.DataFrame({YCOL:yp})
        
    s.to_csv(out,index=False,compression='gzip',float_format='%.6f')
    duration = time.time()-start
    files = [i for i in os.listdir('.') if i.startswith(tag)]
    write_log(duration,score,"{}".format(tag),mfiles=files)

if __name__ == '__main__':
    #main('cv','xgb')
    main('sub','xgb')
    #build('sub',build_list=[1],cache=[1])
